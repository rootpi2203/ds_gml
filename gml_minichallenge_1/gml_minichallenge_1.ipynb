{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "27574a18ec8ec4434b84830de7f4eea6",
     "grade": false,
     "grade_id": "cell-a14a0e42fb32bf6d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# GML - Mini-Challenge 1 - FS 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dff6a9df2010d8480d9e53fdb73bd23d",
     "grade": false,
     "grade_id": "cell-c39ca46dd5223598",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Ausgabe:** Montag, 21. März 2022  \n",
    "**Abgabe:** Sonntag, 24. April 2022, bis 24 Uhr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e67532a9c3c9752ece899065baca52f6",
     "grade": false,
     "grade_id": "cell-c101d4d4ed7daa5f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In dieser Mini-Challenge implementieren und verwenden wir verschiedene Supervised Learning-Methoden und machen Gebrauch von Model Selection-Prinzipien und -Algorithmen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "626f32cc5a7e6b277b2d79f23e86ae1f",
     "grade": false,
     "grade_id": "cell-537cbbbfba1472ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Vorgaben zu Umsetzung und Abgabe\n",
    "\n",
    "- Code muss in python geschrieben werden.\n",
    "- Wir entwickeln zahlreiche Algorithmen selber. Wenn nicht explizit anders verlangt, dürfen bloss die folgenden Bibliotheken verwendet werden: numpy, matplotlib, seaborn, pandas\n",
    "- Der Code muss von Anfang bis Ende lauffähig sein bei Ausführung im Docker-Container des Trainingcenters. Nur was korrekt ausführt wird bewertet.\n",
    "- Es darf kein Code ausgelagert werden.\n",
    "- Sämtliche Plots sind komplett beschriftet (Achsen, Labels, Titel, Colorbar, ..), sodass der Plot einfach verstanden werden kann.\n",
    "- Zu jedem Plot gehört eine kurze Diskussion, welche den Plot erklärt und die wichtigsten Einsichten die damit sichtbar werden festhält.  \n",
    "- Als **Abgabe** zählt der letzte Commit in deinem Fork des Repos vor Abgabetermin.  \n",
    "\n",
    "\n",
    "- **Bitte lösche, dupliziere oder verschiebe die vorhandenen Zellen nicht**. Dies führt zu Problemen bei der Korrektur. Du darfst aber beliebig viele weitere Zellen hinzufügen.\n",
    "- Bitte importiere Daten mit **relativen Pfaden** innerhalb des Repos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2831ec2cdf1c8dc267c5b2557e6715a9",
     "grade": false,
     "grade_id": "cell-a30ee27b9dd26d68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Für die Erarbeitung der Lösung darf unter Studierenden zusammengearbeitet werden. Die Zusammenarbeit ist dabei aber auf konzeptionelle und algorithmische Fragen und Verständnisaspekte beschränkt.  \n",
    "\n",
    "**Es darf kein Code oder Text von anderen oder vom Internet kopiert werden.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c0315ee98213c54058c1b1fbdf5681d",
     "grade": false,
     "grade_id": "cell-444e8bb1dde427ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8e7f6972ace9fae35b9e943ba27612e",
     "grade": false,
     "grade_id": "cell-598892315ccef79b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 1 (4 Punkte)\n",
    "\n",
    "**Ridge Regression** (siehe beispielsweise James et al., *An Introduction to Statistical Learning*, 2015, pp 215) ist eine regularisierte Form ($l_2$-Regularisierung) der Ordinary Least Squares (OLS) Kostenfunktion für die lineare Regression.  \n",
    "\n",
    "Die Ridge Regression-Kostenfunktion für einen Datensatz $(x^{(i)}, y^{(i)})$ mit $x^{(i)} = (x_1^{(i)}, \\dots , x_p^{(i)})$ von $N$ Datenpunkten ist: \n",
    "\n",
    "\\begin{equation}\n",
    "J(\\beta) = \\sum_{i=1}^{N} (y^{(i)}-\\beta_0 - \\sum_{j=1}^{p} x^{(i)}_j\\beta_j)^2 + \\alpha\\sum_{j=1}^{p} \\beta_j^2 \n",
    "\\end{equation}\n",
    "\n",
    "$(\\beta_0, \\beta_1, \\dots, \\beta_p)$ sind dabei die Modellkoeffizienten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7540cd5b9649999e5b06e384583d2af",
     "grade": false,
     "grade_id": "cell-50f0d41613015609",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Es gilt zu beachten, dass bei Ridge Regression der Achsenabschnitt, i.e. der Modellkoeffizient $\\beta_0$ nicht in den Penalty Term der Kostenfunktion eingeht. Das zeigt sich in obiger Gleichung durch die Summe von $i=1$ (nicht $i=0$) bis $p$ (Anzahl Prädiktoren)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "442ebc941f9820d89108fbbaaf900150",
     "grade": false,
     "grade_id": "cell-d1a4ded98458d714",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Für die Optimierung der Koeffizienten bei gegebenem Datensatz ergeben sich dadurch Implikationen für Gradient Descent und Normalengleichung.  \n",
    "\n",
    "Wenn man die Input-Variablen standardisiert, was **bei Regularisierung fast immer angezeigt** ist um sämtliche Variablen auf eine vergleichbare Skala (dimensionslose Standardabweichungen) zu bringen, und damit die zugehörigen Koeffizienten in ähnlichem Umfang zu regularisieren, ist es eine Möglichkeit das Optimierungsproblem für $\\beta_0$ und die restlichen Variablen zu separieren. $\\beta_0$ kann dann nämlich mit $\\beta_0 = \\frac{1}{N} \\sum_{i=1}^{N}y^{(i)}$ berechnet werden und ist so unregularisiert. Die Koeffizienten $(\\beta_1, \\dots, \\beta_p)$ werden dann mit Gradient Descent oder Normalengleichen optimiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a309647b35b0ed51ceebb994b2fdf10",
     "grade": false,
     "grade_id": "cell-969ac96f1888af28",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Möchte man alle Modell-Koeffizienten, inklusive $\\beta_0$, mittels Gradient Descent optimieren, so gilt es eine Fallunterscheidung bei der Berechnung des Gradienten zu machen für die Gradienten-Komponente $0$, welche zum Koeffizienten $\\beta_0$ gehört, und den verbleibenden Gradienten-Komponenten $1$ bis $p$, welche zu den Modellkoeffizienten $(\\beta_1, \\dots, \\beta_p)$ gehören. Dies deswegen, weil $\\beta_0$ nicht in die Strafterm-Summe der Kostenfunktion eingeht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57cf692fe1e428bba285f773b8e29711",
     "grade": false,
     "grade_id": "cell-56f0ecde3f599460",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Zu Gradient Descent für Ridge Regression\n",
    "\n",
    "Zur Verwendung von Gradient Descent muss der Gradient der Kostenfunktion berechnet werden. Der Gradient $\\nabla f(\\chi)$ einer Funktion $f(\\cdot)$ mehrerer ($m$) Variablen $\\chi = (\\chi_1, \\chi_2, \\dots, \\chi_m)$ ist gegeben durch:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla f(\\chi) = \\Big( \\frac{\\partial f(\\chi)}{\\partial \\chi_1}, \\frac{\\partial f(\\chi)}{\\partial \\chi_2}, \\dots, \\frac{\\partial f(\\chi)}{\\partial \\chi_m}\\Big)\n",
    "\\end{equation}\n",
    "\n",
    "$\\frac{\\partial f(\\chi)}{\\partial \\chi_i}$ ist dabei die partielle Ableitung von $f(\\cdot)$ nach $\\chi_i$. $\\nabla f(\\chi)$ ist also ein $m$-dimensionaler Vektor.   \n",
    "\n",
    "Bei Standardisierung der Input-Variablen und separater 'Optimierung' von $\\beta_0$ wird $\\beta_0$ vorab berechnet, wird dann zur Konstanten in der Kostenfunktion, und muss nicht mehr mitoptimiert werden, i.e. kann beim Gradienten aussen vor gelassen werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f474a567191595bd7520380c2b82ff89",
     "grade": false,
     "grade_id": "cell-e9c12302c2d018bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Zur Normalengleichung für Ridge Regression\n",
    "\n",
    "Einen Input-Datensatz können wir als $N \\times p+1$ Matrix $\\mathbf{X}$ schreiben. $p+1$ deswegen, weil wir den $p$ Input-Variablen noch eine Spalte von $1$-en voranstellen können, um den Koeffizienten $\\beta_0$ mit berücksichtigen zu können. Sie hat also die Form\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \n",
    "\\begin{bmatrix}\n",
    "1 & x^{(1)}_1 & x^{(1)}_2 & \\cdots & x^{(1)}_j & \\cdots & x^{(1)}_p \\\\\n",
    "1 & x^{(2)}_1 & x^{(2)}_2 & \\cdots & x^{(2)}_j & \\cdots & x^{(2)}_p \\\\\n",
    "& & \\vdots & &\\\\\n",
    "1 & x^{(i)}_1 & x^{(i)}_2 & \\cdots & x^{(i)}_j & \\cdots & x^{(i)}_p \\\\\n",
    "& & \\vdots & &\\\\\n",
    "1 & x^{(n)}_1 & x^{(n)}_2 & \\cdots & x^{(n)}_j & \\cdots & x^{(n)}_p\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Gleichermassen können wir unsere Output-Variablen als $N$-dimensionalen Vektor $y = (y^{(1)}, .. y^{(N)})$ betrachten.  \n",
    "\n",
    "Damit können wir ein lineares Modell in kompakter Schreibweise wie folgt formulieren:  \n",
    "\n",
    "\\begin{equation}\n",
    "y = \\mathbf{X}\\beta + \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "Wobei $\\epsilon = (\\epsilon_1, \\cdots \\epsilon_N)$ ein Vektor von irreduzierbaren Fehlern für die $N$ Datenpunkte ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9703d4d051aa85305fd5445668a8b73",
     "grade": false,
     "grade_id": "cell-2005988b23517f35",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Für die **unregularisierte OLS Kostenfunktion kann eine analytische Lösung** gefunden werden, die als Normalengleichung bezeichnet wird:\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^Ty\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Gleichermassen kann für die analytische Lösung der **Ridge Regression Kostenfunktion** folgende analytische Lösung hergeleitet werden:\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = (\\mathbf{X}^T \\mathbf{X} + \\alpha \\mathbf{1}_R)^{-1} \\mathbf{X}^Ty\n",
    "\\end{equation}\n",
    "\n",
    "Möchten wir alle Modellkoeffizienten, inklusive $\\beta_0$, auf einmal optimieren, ist $\\mathbf{1}_R$ dabei im Grunde die $(p+1 \\times p+1)$-dimensionale Einheitsmatrix. Allerdings muss das Element $(0,0)$ gleich $0$ gesetzt, dies um den Koeffizienten $\\beta_0$ nicht zu regularisieren. $\\mathbf{1}_R$ ist also:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{1}_r = \n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & \\cdots & 0 & \\cdots & 0 \\\\\n",
    "0 & 1 & 0 & \\cdots & 0 & \\cdots & 0 \\\\\n",
    "0 & 0 & 1 & \\cdots & 0 & \\cdots & 0 \\\\\n",
    "& &  & \\ddots &  & \\vdots &  \\\\\n",
    "& \\vdots &  &  & \\ddots & 0 & 0 \\\\\n",
    "0 & 0 & \\cdots &  & 0 & 1 & 0 \\\\\n",
    "0 & 0 & \\cdots &  &  & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84ec1863eed2091a3b8261026a317879",
     "grade": false,
     "grade_id": "cell-e6a760b2b23174b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Bei standardisierten Inputdaten** kann alternativ auch der oben erwähnte Weg beschritten werden, bei welchem man $\\beta_0 = \\frac{1}{N} \\sum_{i=1}^{N}y^{(i)}$ berechnet, die Spalte von $1$-en $\\mathbf{X}$ nicht vorangestellt und schliesslich eine Normalengleichung verwendet wird mit unveränderter $(p \\times p)$-dimensionaler Einheitsmatrix $\\mathbf{1}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = (\\mathbf{X}^T \\mathbf{X} + \\alpha \\mathbf{1})^{-1} \\mathbf{X}^Ty\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1bfef6aa079c89f70288de03e1908cb",
     "grade": false,
     "grade_id": "cell-9516abc3094115db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Du bist frei in diesem Aufgabenblatt einen beliebigen (korrekten) Weg für Gradient Descent und Normalengleichung zu wählen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc72356e2de44b78f7dfa6573f633b6a",
     "grade": false,
     "grade_id": "cell-7bea1f60917ace51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61d236d1da3db1f11236f8fb8996d68b",
     "grade": false,
     "grade_id": "cell-10d8b9e25ab34dee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Aufgabe**\n",
    "\n",
    "Leite für die obige Ridge Regression-Kostenfunktion den Gradienten und die Normalengleichung analytisch her, für den Fall dass die Inputdaten nicht standardisiert seien.  \n",
    "\n",
    "(Schreibe die Herleitung in LaTex-Notation in die folgende Zelle).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "483340bcb4559a70b175c329bff5928d",
     "grade": true,
     "grade_id": "cell-17b4f6f2b395d4a6",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herleitung der Gradienten Ableitung der Ridge Regression-Kostenfunktion:  \n",
    "\n",
    "Ridge Regression-Kostenfunktion:  \n",
    "\n",
    "\\begin{equation}\n",
    "J(\\beta) = \\sum_{i=1}^{N} \\left(y^{(i)} - \\beta_0 - \\sum_{j=1}^{p}x_{j}^{(i)}\\beta_{j}\\right)^2 +\n",
    "\\alpha\\sum_{j=1}^{p}\\beta_{j}^2\n",
    "\\end{equation}\n",
    "\n",
    "Die partielle Ableitung von $J(\\beta)$ nach $\\frac{\\partial J(\\beta)}{\\partial J_{\\beta k}}$, wobei $\\beta_{k}$ für die einzelnen Modelkoeffizienten steht:  \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "\\frac{\\partial}{\\partial \\beta_k} \n",
    "\\sum_{i=1}^{N} \\left(y^{(i)} - \\beta_0 \\sum_{j=1}^{p} x_j^{(i)} \\beta_j\\right)^2 +\n",
    "\\frac{\\partial}{\\partial \\beta_k} \\alpha \\sum_{j=1}^{p} \\beta_j^2\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Kettenregel:  \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "2 \\sum_{i=1}^{N} \\left((y^{(i)} - \\beta_0 - \\sum_{j=1}^{p} x_j^{(i)} \\beta_j\\right) \\cdot\n",
    "\\frac{\\partial}{\\partial \\beta_k} \n",
    "\\left(y^{(i)} - \\beta_0 - \\sum_{j=1}^{p} x_j^{(i)} \\beta_j \\right) + \n",
    "\\frac{\\partial}{\\partial \\beta_k} \\alpha \\sum_{j=1}^{p} \\beta_j^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "2 \\sum_{i=1}^{N} \\left((y^{(i)} - \\beta_0 - \\sum_{j=1}^{p} x_j^{(i)} \\beta_j) \\cdot\n",
    "(- x_k^{(i)})\\right) +\n",
    "\\frac{\\partial}{\\partial \\beta_k} \\alpha \\sum_{j=1}^{p} \\beta_j^2\n",
    "\\end{equation}\n",
    "\n",
    "2. Summenregel:  \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "2 \\sum_{i=1}^{N} \\left( (y^{(i)} - \\beta_0 - \\sum_{j=1}^{p} x_j^{(i)} \\beta_j) \\cdot\n",
    "\\left(- x_k^{(i)}\\right) \\right) +\n",
    "2 \\alpha \\beta_k\n",
    "\\end{equation}\n",
    "\n",
    "3. (-1) von $-x_k^{(i)}$ voranstellen:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "-2 \\sum_{i=1}^{N} \\left( (y^{(i)} - \\beta_0 - \\sum_{j=1}^{p} x_j^{(i)} \\beta_j) \\cdot\n",
    "\\left(x_k^{(i)}\\right) \\right) +\n",
    "2 \\alpha \\beta_k\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somit gilt für den Gradienten:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla J(\\beta) = \n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "\\left( \\frac{\\partial J(\\beta)}{\\partial \\beta_1},\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_2},\n",
    "\\dots,\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_p}\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herleitung der Normalengleichung der Ridge Regression-Kostenfunktion\n",
    "\n",
    "Ridge Regression-Kostenfunktion:  \n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = (\\mathbf{X}^T \\mathbf{X} + \\alpha \\mathbf{1}_R)^{-1} \\mathbf{X}^Ty\n",
    "\\end{equation}  \n",
    "\n",
    "wobei $\\beta$ ein vektor mit den gesuchten Koeffizienten ($\\beta_1, \\beta_2, \\dots, \\beta_p$) ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Herleitung soll die oben erstellte Formel als Ausgangslage dienen:  \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "-2 \\sum_{i=1}^{N} \\left( (y^{(i)} - \\beta_0 - \\sum_{j=1}^{p} x_j^{(i)} \\beta_j) \\cdot\n",
    "\\left(x_k^{(i)}\\right) \\right) +\n",
    "2 \\alpha \\beta_k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y^{(i)}$ kann als Vektor und $\\beta_0, $ ..  als Matrix dargestellt werden.  \n",
    "\n",
    "\\begin{equation}\n",
    "Y = \\left( \\begin{matrix} y^{(1)} \\\\ y^{(2)} \\\\ y^{(3)} \\\\ \\vdots \\\\ y^{(N)} \\end{matrix} \\right)\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "392fe1fa7c8c3a6a460351b77168014f",
     "grade": false,
     "grade_id": "cell-08d0ecc44ae0095f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 2 (3 Punkte)  \n",
    "\n",
    "Was würde sich in Aufgabe 1 ändern wenn wir anstelle von Ridge Regression Lasso betrachten würden für Kostenfunktion, Koeffizientenoptimierung und Koeffizienten?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b331ef5e94fb0ec53b3aef2bec763ad4",
     "grade": true,
     "grade_id": "cell-cc76f77b20062218",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Lasso Regression unterscheidet sich im Strafterm von der Ridge Regression. Anstelle eines quadratischen Strafterm\n",
    "$\\dots +\\alpha \\sum_{j=1}^{p} \\beta_j^2 $ wird bei Lasso Reg. der Strafterm absolut verwendet \n",
    "$\\dots +\\alpha \\sum_{j=1}^{p} | \\beta_j | $.\n",
    "\n",
    "Lasso Regression-Kostenfunktion:  \n",
    "\n",
    "\\begin{equation}\n",
    "J(\\beta) = \\sum_{i=1}^{N} \\left(y^{(i)} - \\beta_0 - \\sum_{j=1}^{p}x_{j}^{(i)}\\beta_{j}\\right)^2 +\n",
    "\\alpha\\sum_{j=1}^{p} | \\beta_{j}|\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die partielle Ableitung von $J(\\beta)$ nach $\\frac{\\partial J(\\beta)}{\\partial J_{\\beta k}}$, wobei $\\beta_{k}$ für die einzelnen Modelkoeffizienten steht:  \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "2 \\sum_{i=1}^{N} \\left((y^{(i)} - \\beta_0 - \\sum_{j=1}^{p} x_j^{(i)} \\beta_j) \\cdot\n",
    "(- x_k^{(i)})\\right) +\n",
    "\\frac{\\partial}{\\partial \\beta_k} \\alpha \\sum_{j=1}^{p} |\\beta_j|\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "2 \\sum_{i=1}^{N} \\left( (y^{(i)} - \\beta_0 - \\sum_{j=1}^{p} x_j^{(i)} \\beta_j) \\cdot\n",
    "\\left(- x_k^{(i)}\\right) \\right) + \\alpha 1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Änderungen zur Lasso Regression bedeuten für die:\n",
    "1. Kostenfunktion\n",
    "\n",
    "1. Koeffizientenoptimierung\n",
    "\n",
    "1. Koeffizienten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a10c3a4bd82a722085d8867d84333c67",
     "grade": false,
     "grade_id": "cell-bf24b44d2be058ab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 3 (8 Punkte)\n",
    "\n",
    "Komplettiere die folgende Klasse so, dass sie bei Wahl der entsprechenden Initialisierungsoption, das Ausführen der `fit`-Methode die Kostenfunktion der Ridge Regression-Kostenfunktion mit Gradient Descent (`gd`) oder der regularisierten Normalengleichung (`neq`) optimiert.\n",
    "\n",
    "Erstelle nun einen einfachen Datensatz von 1000 Datenpunkten zur Validierung deiner Implementation: Die eindimensionalen x-Werte seien normalverteilt mit Mittelwert 5 und Standardabweichung 3. Die y-Werte seien gegeben durch ein einfaches lineares Modell mit Koeffizienten $\\beta_0=-2$ und $\\beta_1=3$.\n",
    "\n",
    "Zeige damit, dass\n",
    "- deine Implementierung für Gradient Descent erfolgreich konvergiert.\n",
    "- du mit Normalengleichung und Gradient Descent praktisch die gleiche (korrekte) Lösung findest (verwende dazu `np.testing.assert_array_almost_equal`).\n",
    "- der Effekt der Regularisierung sich wie erwartet niederschlägt, wenn du die Werte für die Koeffizienten als Funktion er Regularisierungsstärke $\\alpha$ über den gesamten sinnvollen Bereich für $\\alpha$ zeichnest (Ridge Regression Path). Diskutieren diesen Plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "81d4b425313979972e8e5df27236818a",
     "grade": true,
     "grade_id": "cell-6d8e46bcf1165170",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67e96cbf073d00ed0ae2a560b60e370b",
     "grade": true,
     "grade_id": "cell-e3ecd12d03420258",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class RidgeRegression(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, opt_method='gd', alpha=1., eta=0.01, maxsteps=100, eps=0.00000001):\n",
    "        '''Implements a Ridge Regression estimator.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        alpha:      Regularization proportionality factor. Larger values\n",
    "                    correspond with stronger regularization.\n",
    "        opt_method: Optimization method to choose for the cost function.\n",
    "                    Can be either 'gd' (Gradient Descent) or 'neq'.\n",
    "        maxsteps:   Maximum number of Gradient Descent steps to take.\n",
    "        eps:        Epsilon, length of gradient to be reached with Gradient\n",
    "                    Descent.\n",
    "        eta:        Fixed step lenght to take at each gradient descent\n",
    "                    iteration.\n",
    "        '''\n",
    "        # parameters\n",
    "        self.alpha = alpha\n",
    "        self.opt_method = opt_method\n",
    "        self.maxsteps = maxsteps\n",
    "        self.eps = eps\n",
    "        self.eta = eta\n",
    "        # attributes\n",
    "        # model coefficients\n",
    "        self.beta_ = None\n",
    "        # values of cost function along gradient descent iterations\n",
    "        self.costs_ = []       \n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        '''        \n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        if opt_method == 'gd':\n",
    "            pass\n",
    "            #self.beta_ = \n",
    "        if opt_method == 'neq':\n",
    "            self.beta_ = normalequation(X, y)        \n",
    "        #raise NotImplementedError()\n",
    "        return self\n",
    "        \n",
    "    def gradient_descent(self,X,y):\n",
    "        '''Computes the coefficients of the ridge regression cost function\n",
    "        using gradient descent.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def normalequation(self,X,y):\n",
    "        '''Computes the coefficients of the ridge regression cost function\n",
    "        using the normalequation.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        # erstellen der Einheitsmatrix mit p = Anzahl beta und Element (0,0) = 0 setzen (beta_0 nicht regularisiert)\n",
    "        dim_p = X.shape[1]\n",
    "        one_R = np.ones((dim_p+1, dim_p+1))\n",
    "        one_R[0,0] = 0\n",
    "        # beta Koeffizienten berechnen mit Ridge\n",
    "        beta = np.linalg.inv(X.T @ X + alpha*one_R) @ X.T @ y\n",
    "        return beta\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(beta,X,y,alpha):\n",
    "        '''Computes and returns the gradient of the ridge regression cost function.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @staticmethod \n",
    "    def costfunction(beta,X,y,alpha):\n",
    "        '''Computes and returns the value of the ridge regression cost function.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        y_pred = X @ self.beta_\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    \n",
    "    def predict(self,X):\n",
    "        '''Computes the predictions of the current model.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        if opt_method == 'gd':\n",
    "            pass\n",
    "            #y_predicted =         \n",
    "        if opt_method == 'neq':\n",
    "            y_predicted = costfunction(self.beta_, X, ) #?\n",
    "            \n",
    "        return y_predicted\n",
    "\n",
    "    \n",
    "    def score(self,X,y):\n",
    "        '''Returns R^2 for given input/output data given the model\n",
    "        coefficients.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        # get prediction y\n",
    "        y_predicted = fit(X, y)\n",
    "        # calculate R squared with numpy linear algebra\n",
    "        r2 = 1 - ((y - y_predicted).T @ (y - y_predicted).T / (y - np.average(y).T @ (y - np.average(y))))\n",
    "        \n",
    "        return r2\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def approximate_gradient(beta,X,y,alpha,epsilon=0.00001):\n",
    "        '''Approximates the gradient with finite differences.\n",
    "        \n",
    "        You can use this method to check your gradident.\n",
    "        '''\n",
    "        grad_approx = []\n",
    "        cf = RidgeRegression.costfunction\n",
    "        for i,b in enumerate(beta):\n",
    "            eps = np.zeros(beta.shape[0])\n",
    "            eps[i] += epsilon\n",
    "            print(eps)\n",
    "            grad_approx.append(\n",
    "                (cf(beta+eps,X,y,alpha)-cf(beta-eps,X,y,alpha))/(2*epsilon)\n",
    "            )\n",
    "        return np.array(grad_approx)\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Datensatz\n",
    "np.random.seed(41)\n",
    "mu, sigma, n = 5, 3, 1000\n",
    "x_rand_data = np.random.normal(mu, sigma, size=n)\n",
    "X_rand_data = np.hstack((np.ones(len(x_rand_data)).reshape(-1,1), x_rand_data.reshape(-1,1)))\n",
    "y_rand_data = 3*x_rand_data -2\n",
    "\n",
    "# data plot\n",
    "x_line = np.arange(-5, 15, 1)\n",
    "y_line = 3*x_line -2\n",
    "\n",
    "plt.plot(x_line, y_line, color='red', label='$y = 3x - 2$')\n",
    "plt.scatter(x_rand_data, y_rand_data, label='random data')\n",
    "plt.title(f'Random Daten mu={mu}, std={sigma}, n={n}')\n",
    "plt.xlabel('x'), plt.ylabel('$y = 3x - 2$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "947ac3ace2228d3760ed06421017b046",
     "grade": false,
     "grade_id": "cell-9d564ff80880ce49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 4 (7 Punkte)\n",
    "\n",
    "Schreibe nun eine Funktion, die einen Contour Plot der Kostenfunktion erstellt (siehe dazu gml Aufgabenblatt 1 im Trainingcenter). Zeichne den Pfad von Gradient Descent durch die Koeffizientenebene ein (dazu musst du die obige Klasse modifizieren) und untersuche und vergleiche den Plot für\n",
    "\n",
    "- unterschiedliche Regularisierungsstärken, inkl. ohne Regularisierung.\n",
    "- mit und ohne Standardisierung der Input-Daten.\n",
    "\n",
    "Diskutiere deine Einsichten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bfe62747dc423cc699111b51416f112",
     "grade": true,
     "grade_id": "cell-072994ddc01c2295",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-15b94d1fa268>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# YOUR CODE HERE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d81c379bcd42feca014b58c3e55a476",
     "grade": true,
     "grade_id": "cell-86ed5cdc25c06a1b",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa96bc51bfa2f016eb72be682d745c7e",
     "grade": false,
     "grade_id": "cell-a776cc30034b6f2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 5 (4 Punkte)\n",
    "\n",
    "Lade den Datensatz `data/moto.csv` und verschaffe dir einen Überblick durch explorative Datenanalyse. Unser Ziel wird es sein, den Preis (`price`) der Motorräder vorherzusagen unter Verwendung der übrigen Attribute. Teile deine Überlegungen zu diesem Problem.  \n",
    "\n",
    "Unterteile den Datensatz nun noch in Trainings- und Testdaten (80:20) für die weiteren Aufgaben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c774d9ebfb9cc1b2ef0c7c41aad6f23b",
     "grade": true,
     "grade_id": "cell-6bad7cb791f2cb0c",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af4bbd9d82cef4597f7fa6d7957980bc",
     "grade": true,
     "grade_id": "cell-aa4e32d1f9acea9a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d4d8e3c8e1e7025a8b615a5b871c784",
     "grade": false,
     "grade_id": "cell-028c6bce8cafd826",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 6 (3 Punkte)\n",
    "\n",
    "Erstelle ein erstes einfaches lineares Modell für `price`, bloss mit der einen Input-Variablen `displacement`.  \n",
    "\n",
    "Untersuche für die unregularisierte OLS-Lösung die Modell-Annahmen eines linearen Modells. Schau dir dazu *Kapitel 4 - Residuenanalyse* im Skript von Werner Stahel an, wenn du Anleitung möchtest.    \n",
    "\n",
    "Nimm, falls sinnvoll, Variablen-Transformationen vor, um dein Modell zu verbessern und untersuche den Effekt. Erkläre dein Vorgehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0aca2020fef5382360ab52da7ccd440c",
     "grade": true,
     "grade_id": "cell-1175683b3e469f07",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "502189a5f60da4508329fcc9de0d9e0c",
     "grade": true,
     "grade_id": "cell-7c65d0cdad8c19d1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5416f1a4f1c1069ce7b7e768eb7368b",
     "grade": false,
     "grade_id": "cell-4676bcfe39c11a07",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 7 (4 Punkte)\n",
    "\n",
    "Entwickle nun dein bestes Ridge-Regression-Modell im Sinne von $R^2$ auf dem Trainingsdatensatz. Du darfst durch Feature-Transformation beliebige weitere Attribute hinzufügen. Gebe $R^2$ und MAE auf dem Testdatensatz an.\n",
    "\n",
    "Zur Optimierung der Hyperparameter kannst du scikit-learn-Funktionalität verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3600764a10f6f837ae5384df5857b2f9",
     "grade": true,
     "grade_id": "cell-c8b2568f340bda1a",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec7af4bd23f868ab39375b2654f983ae",
     "grade": true,
     "grade_id": "cell-dae4f34d368fcdeb",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c48b7faf9e4ff712ee0f8b34df3a1909",
     "grade": false,
     "grade_id": "cell-f039db7c5f629d17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 8 (4 Punkte)\n",
    "\n",
    "Erstelle einen Plot bei welchem du auf der x-Achse die Regularisierungsstärke $\\alpha$ und auf der y-Achse $R^2$ für Trainings- und Testdaten (zwei Kurven) zeichnest. Diskutiere den Plot hinsichtlich Bias-Variance Trade-Off und der Verallgemeinerungsfähigkeit des Modells.  \n",
    "\n",
    "Was ziehst du daraus für Schlüsse für weitere Modellierungsschritte?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cdd2511cc68f17a6d93dad95f9140534",
     "grade": true,
     "grade_id": "cell-3c7f182708f86c2a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d14f655c895df26e63139c013216c54",
     "grade": true,
     "grade_id": "cell-19c5f45de63f73c9",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3782c2247e947870863bd49395f6a8f",
     "grade": false,
     "grade_id": "cell-76d35d0e51f7a3ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 9 (10 Punkte)\n",
    "\n",
    "Was ist das beste Modell für die Output-Variable `price` im Sinne von $R^2$, das du ohne Einschränkungen finden kannst?\n",
    "\n",
    "Vergleiche dazu mindestens drei weitere Ansätze miteinander.\n",
    "\n",
    "Wie verändert sich die Situation wenn du für den *Mean Absolute Error* (MAE) optimierst?\n",
    "\n",
    "Hierzu kannst du scikit-learn Funktionalität verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dafc6ca9eec22c9e7840df949f6180e3",
     "grade": true,
     "grade_id": "cell-76d9178f80e91550",
     "locked": false,
     "points": 8,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "557b204ea14ff8ce0fb40fa3dc0b8d1a",
     "grade": true,
     "grade_id": "cell-f49d6edea9f585c9",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02e93b46e9f8e5a2eb4bb89de79d2bbb",
     "grade": false,
     "grade_id": "cell-deb1a332cd0f2c86",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 10 (4 Punkte)\n",
    "\n",
    "Stelle nun die Resultate der bisherigen Aufgaben und Modelle tabellarisch und graphisch übersichtlich dar und diskutiere sie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f3dfc525169dec929a7e0e67c72b9b1",
     "grade": true,
     "grade_id": "cell-fc430cac16df0d88",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf40ff2cf8567935ebffe998ca4b4db3",
     "grade": true,
     "grade_id": "cell-90051cdc8eaa5d18",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3e906472eee2f6993bee2fee4e52df9",
     "grade": false,
     "grade_id": "cell-e36873df0c0262e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 11 (8 Punkte)\n",
    "\n",
    "Nun betrachten wir noch dein bestes Modell etwas vertieft. Trainiere diesen Modell-Ansatz auf jeweils $[\\frac{1}{10}, \\frac{2}{10}, \\frac{3}{10}, .., \\frac{10}{10}]$ der Trainingsdaten. Erstelle nun einen Plot bei welchem der Wert der Kostenfunktion für die Trainings- und Testdaten auf der y-Achse und der Trainingsdatenanteil auf der x-Achse liegen möge. Zeichne also zwei Kurven in dieses Koordinatensystem.\n",
    "\n",
    "Schau dir dazu das Video von [Kilian Weinberger zu Model Selection](https://www.youtube.com/watch?v=a7cofmFgwIk&list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS&index=22) an.  \n",
    "\n",
    "Interpretiere und diskutiere nun deine Einsichten zu Model Selection, Bias & Variance und Grösse des Datensatzes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48f8a2cbb4285cf7423a6afcf418b578",
     "grade": true,
     "grade_id": "cell-b5a7adbdf7bdd16c",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e62f43ee707f43f2ad20b107985f0f5a",
     "grade": true,
     "grade_id": "cell-f51cd5301642eb12",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
