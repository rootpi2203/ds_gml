{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "27574a18ec8ec4434b84830de7f4eea6",
     "grade": false,
     "grade_id": "cell-a14a0e42fb32bf6d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# GML - Mini-Challenge 1 - FS 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dff6a9df2010d8480d9e53fdb73bd23d",
     "grade": false,
     "grade_id": "cell-c39ca46dd5223598",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Ausgabe:** Montag, 21. März 2022  \n",
    "**Abgabe:** Sonntag, 24. April 2022, bis 24 Uhr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e67532a9c3c9752ece899065baca52f6",
     "grade": false,
     "grade_id": "cell-c101d4d4ed7daa5f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In dieser Mini-Challenge implementieren und verwenden wir verschiedene Supervised Learning-Methoden und machen Gebrauch von Model Selection-Prinzipien und -Algorithmen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "626f32cc5a7e6b277b2d79f23e86ae1f",
     "grade": false,
     "grade_id": "cell-537cbbbfba1472ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Vorgaben zu Umsetzung und Abgabe\n",
    "\n",
    "- Code muss in python geschrieben werden.\n",
    "- Wir entwickeln zahlreiche Algorithmen selber. Wenn nicht explizit anders verlangt, dürfen bloss die folgenden Bibliotheken verwendet werden: numpy, matplotlib, seaborn, pandas\n",
    "- Der Code muss von Anfang bis Ende lauffähig sein bei Ausführung im Docker-Container des Trainingcenters. Nur was korrekt ausführt wird bewertet.\n",
    "- Es darf kein Code ausgelagert werden.\n",
    "- Sämtliche Plots sind komplett beschriftet (Achsen, Labels, Titel, Colorbar, ..), sodass der Plot einfach verstanden werden kann.\n",
    "- Zu jedem Plot gehört eine kurze Diskussion, welche den Plot erklärt und die wichtigsten Einsichten die damit sichtbar werden festhält.  \n",
    "- Als **Abgabe** zählt der letzte Commit in deinem Fork des Repos vor Abgabetermin.  \n",
    "\n",
    "\n",
    "- **Bitte lösche, dupliziere oder verschiebe die vorhandenen Zellen nicht**. Dies führt zu Problemen bei der Korrektur. Du darfst aber beliebig viele weitere Zellen hinzufügen.\n",
    "- Bitte importiere Daten mit **relativen Pfaden** innerhalb des Repos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2831ec2cdf1c8dc267c5b2557e6715a9",
     "grade": false,
     "grade_id": "cell-a30ee27b9dd26d68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Für die Erarbeitung der Lösung darf unter Studierenden zusammengearbeitet werden. Die Zusammenarbeit ist dabei aber auf konzeptionelle und algorithmische Fragen und Verständnisaspekte beschränkt.  \n",
    "\n",
    "**Es darf kein Code oder Text von anderen oder vom Internet kopiert werden.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c0315ee98213c54058c1b1fbdf5681d",
     "grade": false,
     "grade_id": "cell-444e8bb1dde427ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8e7f6972ace9fae35b9e943ba27612e",
     "grade": false,
     "grade_id": "cell-598892315ccef79b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 1 (4 Punkte)\n",
    "\n",
    "**Ridge Regression** (siehe beispielsweise James et al., *An Introduction to Statistical Learning*, 2015, pp 215) ist eine regularisierte Form ($l_2$-Regularisierung) der Ordinary Least Squares (OLS) Kostenfunktion für die lineare Regression.  \n",
    "\n",
    "Die Ridge Regression-Kostenfunktion für einen Datensatz $(x^{(i)}, y^{(i)})$ mit $x^{(i)} = (x_1^{(i)}, \\dots , x_p^{(i)})$ von $N$ Datenpunkten ist: \n",
    "\n",
    "\\begin{equation}\n",
    "J(\\beta) = \\sum_{i=1}^{N} (y^{(i)}-\\beta_0 - \\sum_{j=1}^{p} x^{(i)}_j\\beta_j)^2 + \\alpha\\sum_{j=1}^{p} \\beta_j^2 \n",
    "\\end{equation}\n",
    "\n",
    "$(\\beta_0, \\beta_1, \\dots, \\beta_p)$ sind dabei die Modellkoeffizienten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7540cd5b9649999e5b06e384583d2af",
     "grade": false,
     "grade_id": "cell-50f0d41613015609",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Es gilt zu beachten, dass bei Ridge Regression der Achsenabschnitt, i.e. der Modellkoeffizient $\\beta_0$ nicht in den Penalty Term der Kostenfunktion eingeht. Das zeigt sich in obiger Gleichung durch die Summe von $i=1$ (nicht $i=0$) bis $p$ (Anzahl Prädiktoren)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "442ebc941f9820d89108fbbaaf900150",
     "grade": false,
     "grade_id": "cell-d1a4ded98458d714",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Für die Optimierung der Koeffizienten bei gegebenem Datensatz ergeben sich dadurch Implikationen für Gradient Descent und Normalengleichung.  \n",
    "\n",
    "Wenn man die Input-Variablen standardisiert, was **bei Regularisierung fast immer angezeigt** ist um sämtliche Variablen auf eine vergleichbare Skala (dimensionslose Standardabweichungen) zu bringen, und damit die zugehörigen Koeffizienten in ähnlichem Umfang zu regularisieren, ist es eine Möglichkeit das Optimierungsproblem für $\\beta_0$ und die restlichen Variablen zu separieren. $\\beta_0$ kann dann nämlich mit $\\beta_0 = \\frac{1}{N} \\sum_{i=1}^{N}y^{(i)}$ berechnet werden und ist so unregularisiert. Die Koeffizienten $(\\beta_1, \\dots, \\beta_p)$ werden dann mit Gradient Descent oder Normalengleichen optimiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a309647b35b0ed51ceebb994b2fdf10",
     "grade": false,
     "grade_id": "cell-969ac96f1888af28",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Möchte man alle Modell-Koeffizienten, inklusive $\\beta_0$, mittels Gradient Descent optimieren, so gilt es eine Fallunterscheidung bei der Berechnung des Gradienten zu machen für die Gradienten-Komponente $0$, welche zum Koeffizienten $\\beta_0$ gehört, und den verbleibenden Gradienten-Komponenten $1$ bis $p$, welche zu den Modellkoeffizienten $(\\beta_1, \\dots, \\beta_p)$ gehören. Dies deswegen, weil $\\beta_0$ nicht in die Strafterm-Summe der Kostenfunktion eingeht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57cf692fe1e428bba285f773b8e29711",
     "grade": false,
     "grade_id": "cell-56f0ecde3f599460",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Zu Gradient Descent für Ridge Regression\n",
    "\n",
    "Zur Verwendung von Gradient Descent muss der Gradient der Kostenfunktion berechnet werden. Der Gradient $\\nabla f(\\chi)$ einer Funktion $f(\\cdot)$ mehrerer ($m$) Variablen $\\chi = (\\chi_1, \\chi_2, \\dots, \\chi_m)$ ist gegeben durch:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla f(\\chi) = \\Big( \\frac{\\partial f(\\chi)}{\\partial \\chi_1}, \\frac{\\partial f(\\chi)}{\\partial \\chi_2}, \\dots, \\frac{\\partial f(\\chi)}{\\partial \\chi_m}\\Big)\n",
    "\\end{equation}\n",
    "\n",
    "$\\frac{\\partial f(\\chi)}{\\partial \\chi_i}$ ist dabei die partielle Ableitung von $f(\\cdot)$ nach $\\chi_i$. $\\nabla f(\\chi)$ ist also ein $m$-dimensionaler Vektor.   \n",
    "\n",
    "Bei Standardisierung der Input-Variablen und separater 'Optimierung' von $\\beta_0$ wird $\\beta_0$ vorab berechnet, wird dann zur Konstanten in der Kostenfunktion, und muss nicht mehr mitoptimiert werden, i.e. kann beim Gradienten aussen vor gelassen werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f474a567191595bd7520380c2b82ff89",
     "grade": false,
     "grade_id": "cell-e9c12302c2d018bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Zur Normalengleichung für Ridge Regression\n",
    "\n",
    "Einen Input-Datensatz können wir als $N \\times p+1$ Matrix $\\mathbf{X}$ schreiben. $p+1$ deswegen, weil wir den $p$ Input-Variablen noch eine Spalte von $1$-en voranstellen können, um den Koeffizienten $\\beta_0$ mit berücksichtigen zu können. Sie hat also die Form\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \n",
    "\\begin{bmatrix}\n",
    "1 & x^{(1)}_1 & x^{(1)}_2 & \\cdots & x^{(1)}_j & \\cdots & x^{(1)}_p \\\\\n",
    "1 & x^{(2)}_1 & x^{(2)}_2 & \\cdots & x^{(2)}_j & \\cdots & x^{(2)}_p \\\\\n",
    "& & \\vdots & &\\\\\n",
    "1 & x^{(i)}_1 & x^{(i)}_2 & \\cdots & x^{(i)}_j & \\cdots & x^{(i)}_p \\\\\n",
    "& & \\vdots & &\\\\\n",
    "1 & x^{(n)}_1 & x^{(n)}_2 & \\cdots & x^{(n)}_j & \\cdots & x^{(n)}_p\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Gleichermassen können wir unsere Output-Variablen als $N$-dimensionalen Vektor $y = (y^{(1)}, .. y^{(N)})$ betrachten.  \n",
    "\n",
    "Damit können wir ein lineares Modell in kompakter Schreibweise wie folgt formulieren:  \n",
    "\n",
    "\\begin{equation}\n",
    "y = \\mathbf{X}\\beta + \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "Wobei $\\epsilon = (\\epsilon_1, \\cdots \\epsilon_N)$ ein Vektor von irreduzierbaren Fehlern für die $N$ Datenpunkte ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9703d4d051aa85305fd5445668a8b73",
     "grade": false,
     "grade_id": "cell-2005988b23517f35",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Für die **unregularisierte OLS Kostenfunktion kann eine analytische Lösung** gefunden werden, die als Normalengleichung bezeichnet wird:\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^Ty\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Gleichermassen kann für die analytische Lösung der **Ridge Regression Kostenfunktion** folgende analytische Lösung hergeleitet werden:\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = (\\mathbf{X}^T \\mathbf{X} + \\alpha \\mathbf{1}_R)^{-1} \\mathbf{X}^Ty\n",
    "\\end{equation}\n",
    "\n",
    "Möchten wir alle Modellkoeffizienten, inklusive $\\beta_0$, auf einmal optimieren, ist $\\mathbf{1}_R$ dabei im Grunde die $(p+1 \\times p+1)$-dimensionale Einheitsmatrix. Allerdings muss das Element $(0,0)$ gleich $0$ gesetzt, dies um den Koeffizienten $\\beta_0$ nicht zu regularisieren. $\\mathbf{1}_R$ ist also:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{1}_r = \n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & \\cdots & 0 & \\cdots & 0 \\\\\n",
    "0 & 1 & 0 & \\cdots & 0 & \\cdots & 0 \\\\\n",
    "0 & 0 & 1 & \\cdots & 0 & \\cdots & 0 \\\\\n",
    "& &  & \\ddots &  & \\vdots &  \\\\\n",
    "& \\vdots &  &  & \\ddots & 0 & 0 \\\\\n",
    "0 & 0 & \\cdots &  & 0 & 1 & 0 \\\\\n",
    "0 & 0 & \\cdots &  &  & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84ec1863eed2091a3b8261026a317879",
     "grade": false,
     "grade_id": "cell-e6a760b2b23174b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Bei standardisierten Inputdaten** kann alternativ auch der oben erwähnte Weg beschritten werden, bei welchem man $\\beta_0 = \\frac{1}{N} \\sum_{i=1}^{N}y^{(i)}$ berechnet, die Spalte von $1$-en $\\mathbf{X}$ nicht vorangestellt und schliesslich eine Normalengleichung verwendet wird mit unveränderter $(p \\times p)$-dimensionaler Einheitsmatrix $\\mathbf{1}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = (\\mathbf{X}^T \\mathbf{X} + \\alpha \\mathbf{1})^{-1} \\mathbf{X}^Ty\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1bfef6aa079c89f70288de03e1908cb",
     "grade": false,
     "grade_id": "cell-9516abc3094115db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Du bist frei in diesem Aufgabenblatt einen beliebigen (korrekten) Weg für Gradient Descent und Normalengleichung zu wählen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc72356e2de44b78f7dfa6573f633b6a",
     "grade": false,
     "grade_id": "cell-7bea1f60917ace51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61d236d1da3db1f11236f8fb8996d68b",
     "grade": false,
     "grade_id": "cell-10d8b9e25ab34dee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Aufgabe**\n",
    "\n",
    "Leite für die obige Ridge Regression-Kostenfunktion den Gradienten und die Normalengleichung analytisch her, für den Fall dass die Inputdaten nicht standardisiert seien.  \n",
    "\n",
    "(Schreibe die Herleitung in LaTex-Notation in die folgende Zelle).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "483340bcb4559a70b175c329bff5928d",
     "grade": true,
     "grade_id": "cell-17b4f6f2b395d4a6",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herleitung der **Gradienten Ableitung** der Ridge Regression-Kostenfunktion:  \n",
    "\n",
    "Ridge Regression-Kostenfunktion:  \n",
    "\n",
    "\\begin{equation}\n",
    "J(\\beta) = \\sum_{i=1}^{N} \\left(y^{(i)} - \\beta_0 - \\sum_{j=1}^{p}x_{j}^{(i)}\\beta_{j}\\right)^2 +\n",
    "\\alpha\\sum_{j=1}^{p}\\beta_{j}^2\n",
    "\\end{equation}\n",
    "\n",
    "Die partielle Ableitung von $J(\\beta)$ nach $\\frac{\\partial J(\\beta)}{\\partial J_{\\beta k}}$, wobei $\\beta_{k}$ für die einzelnen Modelkoeffizienten steht:  \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "\\frac{\\partial}{\\partial \\beta_k} \n",
    "\\sum_{i=1}^{N} \\left(y^{(i)} - \\beta_0 \\sum_{j=1}^{p} x_j^{(i)} \\beta_j\\right)^2 +\n",
    "\\frac{\\partial}{\\partial \\beta_k} \\alpha \\sum_{j=1}^{p} \\beta_j^2\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Kettenregel:  \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "2 \\sum_{i=1}^{N} \\left((y^{(i)} - \\beta_0 - \\sum_{j=1}^{p} x_j^{(i)} \\beta_j\\right) \\cdot\n",
    "\\frac{\\partial}{\\partial \\beta_k} \n",
    "\\left(y^{(i)} - \\beta_0 - \\sum_{j=1}^{p} x_j^{(i)} \\beta_j \\right) + \n",
    "\\frac{\\partial}{\\partial \\beta_k} \\alpha \\sum_{j=1}^{p} \\beta_j^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "2 \\sum_{i=1}^{N} \\left((y^{(i)} - \\beta_0 - \\sum_{j=1}^{p} x_j^{(i)} \\beta_j) \\cdot\n",
    "(- x_k^{(i)})\\right) +\n",
    "\\frac{\\partial}{\\partial \\beta_k} \\alpha \\sum_{j=1}^{p} \\beta_j^2\n",
    "\\end{equation}\n",
    "\n",
    "2. Summenregel:  \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "2 \\sum_{i=1}^{N} \\left( (y^{(i)} - \\beta_0 - \\sum_{j=1}^{p} x_j^{(i)} \\beta_j) \\cdot\n",
    "\\left(- x_k^{(i)}\\right) \\right) +\n",
    "2 \\alpha \\beta_k\n",
    "\\end{equation}\n",
    "\n",
    "3. (-1) von $-x_k^{(i)}$ voranstellen:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "-2 \\sum_{i=1}^{N} \\left( (y^{(i)} - \\beta_0 - \\sum_{j=1}^{p} x_j^{(i)} \\beta_j) \\cdot\n",
    "\\left(x_k^{(i)}\\right) \\right) +\n",
    "2 \\alpha \\beta_k\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somit gilt für den Gradienten:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla J(\\beta) = \n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "\\left( \\frac{\\partial J(\\beta)}{\\partial \\beta_1},\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_2},\n",
    "\\dots,\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_p}\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herleitung der **Normalengleichung** der Ridge Regression-Kostenfunktion\n",
    "\n",
    "Ridge Regression-Kostenfunktion:  \n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = (\\mathbf{X}^T \\mathbf{X} + \\alpha \\mathbf{1}_R)^{-1} \\mathbf{X}^Ty\n",
    "\\end{equation}  \n",
    "\n",
    "wobei $\\beta$ ein vektor mit den gesuchten Modellkoeffizienten ($\\beta_1, \\beta_2, \\dots, \\beta_p$) ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Herleitung soll die oben erstellte Formel als Ausgangslage dienen:  \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "-2 \\sum_{i=1}^{N} \\left( (y^{(i)} - \\beta_0 - \\sum_{j=1}^{p} x_j^{(i)} \\beta_j) \\cdot\n",
    "\\left(x_k^{(i)}\\right) \\right) +\n",
    "2 \\alpha \\beta_k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y$ ($y^{(1)}, y^{(2)},\\dots, y^{(N)}$) und $\\beta$ ($(\\beta_0, \\beta_1, \\dots, \\beta_p$) als Vektoren für y und die Modellkoeffizienten. Aus der Aufgabenstellung werden die Inputdaten als Matrix X dargestellt. Somit kann die Summenschreibweise als Ausdruck von Vektoren und Matrizen umgeschrieben werden.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "-2 \\sum_{i=1}^{N} \\left( x_k^{(i)} \\cdot y^{(i)} - \n",
    "x_k^{(i)} \\cdot \\beta_0 - \n",
    "\\sum_{j=1}^{p} x_k^{(i)} \\cdot x_j^{(i)} \\beta_j) \\right) +\n",
    "2 \\alpha \\beta_k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summenschreibweise mit Vektoren und Matrizen ersetzt:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "-2( X^Ty - X^TX\\beta)  +2 \\alpha 1_R\\beta\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "-2X^T (y - X\\beta)  +2 \\alpha 1_R\\beta\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für 'least squares' wird der Gradient mit Wert $0$ gesucht (mit der Hesse Matrix könnte bewiesen  werden, das es sich um ein globales Minimum handelt (Hesse = positiv definit):\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla J(\\beta) = \n",
    "0 = -2X^Ty + 2X^TX\\beta +2 \\alpha 1_R\\beta\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "X^Ty = X^TX\\beta +\\alpha 1_R\\beta\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "X^Ty = (X^TX +\\alpha 1_R) \\beta\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach dem freistellen von $\\beta$ erhält man die Normalengleichung der Aufgabenstellung:  \n",
    "\n",
    "\\begin{equation}\n",
    "(X^TX +\\alpha 1_R)^{-1}X^Ty =  \\beta\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "392fe1fa7c8c3a6a460351b77168014f",
     "grade": false,
     "grade_id": "cell-08d0ecc44ae0095f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 2 (3 Punkte)  \n",
    "\n",
    "Was würde sich in Aufgabe 1 ändern wenn wir anstelle von Ridge Regression Lasso betrachten würden für Kostenfunktion, Koeffizientenoptimierung und Koeffizienten?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b331ef5e94fb0ec53b3aef2bec763ad4",
     "grade": true,
     "grade_id": "cell-cc76f77b20062218",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Lasso Regression unterscheidet sich im Strafterm von der Ridge Regression. Anstelle eines quadratischen Strafterm\n",
    "$\\dots +\\alpha \\sum_{j=1}^{p} \\beta_j^2 $ wird bei Lasso Regression der Strafterm absolut verwendet \n",
    "$\\dots +\\alpha \\sum_{j=1}^{p} | \\beta_j | $.\n",
    "\n",
    "Lasso Regression-Kostenfunktion:  \n",
    "\n",
    "\\begin{equation}\n",
    "J(\\beta) = \\sum_{i=1}^{N} \\left(y^{(i)} - \\beta_0 - \\sum_{j=1}^{p}x_{j}^{(i)}\\beta_{j}\\right)^2 +\n",
    "\\alpha\\sum_{j=1}^{p} | \\beta_{j}|\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die partielle Ableitung von $J(\\beta)$ nach $\\frac{\\partial J(\\beta)}{\\partial J_{\\beta k}}$, wobei $\\beta_{k}$ für die einzelnen Modelkoeffizienten ändert sich folgt:  \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "-2 \\sum_{i=1}^{N} \\left((y^{(i)} - \\beta_0 - \\sum_{j=1}^{p} x_j^{(i)} \\beta_j) \\cdot\n",
    "(x_k^{(i)})\\right) +\n",
    "\\frac{\\partial}{\\partial \\beta_k} \\alpha \\sum_{j=1}^{p} |\\beta_j|\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \n",
    "-2 \\sum_{i=1}^{N} \\left( (y^{(i)} - \\beta_0 - \\sum_{j=1}^{p} x_j^{(i)} \\beta_j) \\cdot\n",
    "\\left(x_k^{(i)}\\right) \\right) + \\alpha 1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Änderungen zur Lasso Regression bedeuten für die:\n",
    "1. Kostenfunktion\n",
    "\n",
    "1. Koeffizientenoptimierung\n",
    "\n",
    "1. Koeffizienten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a10c3a4bd82a722085d8867d84333c67",
     "grade": false,
     "grade_id": "cell-bf24b44d2be058ab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 3 (8 Punkte)\n",
    "\n",
    "Komplettiere die folgende Klasse so, dass sie bei Wahl der entsprechenden Initialisierungsoption, das Ausführen der `fit`-Methode die Kostenfunktion der Ridge Regression-Kostenfunktion mit Gradient Descent (`gd`) oder der regularisierten Normalengleichung (`neq`) optimiert.\n",
    "\n",
    "Erstelle nun einen einfachen Datensatz von 1000 Datenpunkten zur Validierung deiner Implementation: Die eindimensionalen x-Werte seien normalverteilt mit Mittelwert 5 und Standardabweichung 3. Die y-Werte seien gegeben durch ein einfaches lineares Modell mit Koeffizienten $\\beta_0=-2$ und $\\beta_1=3$.\n",
    "\n",
    "Zeige damit, dass\n",
    "- deine Implementierung für Gradient Descent erfolgreich konvergiert.\n",
    "- du mit Normalengleichung und Gradient Descent praktisch die gleiche (korrekte) Lösung findest (verwende dazu `np.testing.assert_array_almost_equal`).\n",
    "- der Effekt der Regularisierung sich wie erwartet niederschlägt, wenn du die Werte für die Koeffizienten als Funktion er Regularisierungsstärke $\\alpha$ über den gesamten sinnvollen Bereich für $\\alpha$ zeichnest (Ridge Regression Path). Diskutieren diesen Plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "81d4b425313979972e8e5df27236818a",
     "grade": true,
     "grade_id": "cell-6d8e46bcf1165170",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67e96cbf073d00ed0ae2a560b60e370b",
     "grade": true,
     "grade_id": "cell-e3ecd12d03420258",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class RidgeRegression(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, opt_method='gd', alpha=1., eta=0.01, maxsteps=100, eps=0.00000001):\n",
    "        '''Implements a Ridge Regression estimator.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        alpha:      Regularization proportionality factor. Larger values\n",
    "                    correspond with stronger regularization.\n",
    "        opt_method: Optimization method to choose for the cost function.\n",
    "                    Can be either 'gd' (Gradient Descent) or 'neq'.\n",
    "        maxsteps:   Maximum number of Gradient Descent steps to take.\n",
    "        eps:        Epsilon, length of gradient to be reached with Gradient\n",
    "                    Descent.\n",
    "        eta:        Fixed step lenght to take at each gradient descent\n",
    "                    iteration.\n",
    "        '''\n",
    "        # parameters\n",
    "        self.alpha = alpha\n",
    "        self.opt_method = opt_method\n",
    "        self.maxsteps = maxsteps\n",
    "        self.eps = eps\n",
    "        self.eta = eta\n",
    "        # attributes\n",
    "        # model coefficients\n",
    "        self.beta_ = None\n",
    "        # values of cost function along gradient descent iterations\n",
    "        self.costs_ = []       \n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        '''        \n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        if opt_method == 'gd':\n",
    "            pass\n",
    "            #self.beta_ = \n",
    "        if opt_method == 'neq':\n",
    "            self.beta_ = self.normalequation(X, y)        \n",
    "        #raise NotImplementedError()\n",
    "        return self\n",
    "        \n",
    "    def gradient_descent(self,X,y):\n",
    "        '''Computes the coefficients of the ridge regression cost function\n",
    "        using gradient descent.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def normalequation(self,X,y):\n",
    "        '''Computes the coefficients of the ridge regression cost function\n",
    "        using the normalequation.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        # erstellen der Einheitsmatrix mit p = Anzahl beta und Element (0,0) = 0 setzen (beta_0 nicht regularisiert)\n",
    "        dim_p = X.shape[1]\n",
    "        one_R = np.ones((dim_p+1, dim_p+1))\n",
    "        one_R[0,0] = 0\n",
    "        # beta Koeffizienten berechnen mit Ridge\n",
    "        beta = np.linalg.inv(X.T @ X + alpha*one_R) @ X.T @ y\n",
    "        return beta\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(beta,X,y,alpha):\n",
    "        '''Computes and returns the gradient of the ridge regression cost function.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @staticmethod \n",
    "    def costfunction(beta,X,y,alpha):\n",
    "        '''Computes and returns the value of the ridge regression cost function.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        y_pred = X @ self.beta_\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    \n",
    "    def predict(self,X):\n",
    "        '''Computes the predictions of the current model.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        if opt_method == 'gd':\n",
    "            pass\n",
    "            #y_predicted =         \n",
    "        if opt_method == 'neq':\n",
    "            y_predicted = costfunction(self.beta_, X, ) #?\n",
    "            \n",
    "        return y_predicted\n",
    "\n",
    "    \n",
    "    def score(self,X,y):\n",
    "        '''Returns R^2 for given input/output data given the model\n",
    "        coefficients.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        # get prediction y\n",
    "        y_predicted = fit(X, y)\n",
    "        # calculate R squared with numpy linear algebra\n",
    "        r2 = 1 - ((y - y_predicted).T @ (y - y_predicted).T / (y - np.average(y).T @ (y - np.average(y))))\n",
    "        \n",
    "        return r2\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def approximate_gradient(beta,X,y,alpha,epsilon=0.00001):\n",
    "        '''Approximates the gradient with finite differences.\n",
    "        \n",
    "        You can use this method to check your gradident.\n",
    "        '''\n",
    "        grad_approx = []\n",
    "        cf = RidgeRegression.costfunction\n",
    "        for i,b in enumerate(beta):\n",
    "            eps = np.zeros(beta.shape[0])\n",
    "            eps[i] += epsilon\n",
    "            print(eps)\n",
    "            grad_approx.append(\n",
    "                (cf(beta+eps,X,y,alpha)-cf(beta-eps,X,y,alpha))/(2*epsilon)\n",
    "            )\n",
    "        return np.array(grad_approx)\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Datensatz\n",
    "np.random.seed(41)\n",
    "mu, sigma, n = 5, 3, 1000\n",
    "x_rand_data = np.random.normal(mu, sigma, size=n)\n",
    "X_rand_data = np.hstack((np.ones(len(x_rand_data)).reshape(-1,1), x_rand_data.reshape(-1,1)))\n",
    "y_rand_data = 3*x_rand_data -2 + np.random.normal(0, 3, n)\n",
    "\n",
    "# data plot\n",
    "x_line = np.arange(-5, 15, 1)\n",
    "y_line = 3*x_line -2\n",
    "\n",
    "plt.plot(x_line, y_line, color='red', label='$y = 3x - 2$')\n",
    "plt.scatter(x_rand_data, y_rand_data, label='random data')\n",
    "plt.title(f'Random Daten mu={mu}, std={sigma}, n={n}')\n",
    "plt.xlabel('x'), plt.ylabel('$y = 3x - 2$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "947ac3ace2228d3760ed06421017b046",
     "grade": false,
     "grade_id": "cell-9d564ff80880ce49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 4 (7 Punkte)\n",
    "\n",
    "Schreibe nun eine Funktion, die einen Contour Plot der Kostenfunktion erstellt (siehe dazu gml Aufgabenblatt 1 im Trainingcenter). Zeichne den Pfad von Gradient Descent durch die Koeffizientenebene ein (dazu musst du die obige Klasse modifizieren) und untersuche und vergleiche den Plot für\n",
    "\n",
    "- unterschiedliche Regularisierungsstärken, inkl. ohne Regularisierung.\n",
    "- mit und ohne Standardisierung der Input-Daten.\n",
    "\n",
    "Diskutiere deine Einsichten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bfe62747dc423cc699111b51416f112",
     "grade": true,
     "grade_id": "cell-072994ddc01c2295",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d81c379bcd42feca014b58c3e55a476",
     "grade": true,
     "grade_id": "cell-86ed5cdc25c06a1b",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa96bc51bfa2f016eb72be682d745c7e",
     "grade": false,
     "grade_id": "cell-a776cc30034b6f2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 5 (4 Punkte)\n",
    "\n",
    "Lade den Datensatz `data/moto.csv` und verschaffe dir einen Überblick durch explorative Datenanalyse. Unser Ziel wird es sein, den Preis (`price`) der Motorräder vorherzusagen unter Verwendung der übrigen Attribute. Teile deine Überlegungen zu diesem Problem.  \n",
    "\n",
    "Unterteile den Datensatz nun noch in Trainings- und Testdaten (80:20) für die weiteren Aufgaben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c774d9ebfb9cc1b2ef0c7c41aad6f23b",
     "grade": true,
     "grade_id": "cell-6bad7cb791f2cb0c",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af4bbd9d82cef4597f7fa6d7957980bc",
     "grade": true,
     "grade_id": "cell-aa4e32d1f9acea9a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d4d8e3c8e1e7025a8b615a5b871c784",
     "grade": false,
     "grade_id": "cell-028c6bce8cafd826",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 6 (3 Punkte)\n",
    "\n",
    "Erstelle ein erstes einfaches lineares Modell für `price`, bloss mit der einen Input-Variablen `displacement`.  \n",
    "\n",
    "Untersuche für die unregularisierte OLS-Lösung die Modell-Annahmen eines linearen Modells. Schau dir dazu *Kapitel 4 - Residuenanalyse* im Skript von Werner Stahel an, wenn du Anleitung möchtest.    \n",
    "\n",
    "Nimm, falls sinnvoll, Variablen-Transformationen vor, um dein Modell zu verbessern und untersuche den Effekt. Erkläre dein Vorgehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0aca2020fef5382360ab52da7ccd440c",
     "grade": true,
     "grade_id": "cell-1175683b3e469f07",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "502189a5f60da4508329fcc9de0d9e0c",
     "grade": true,
     "grade_id": "cell-7c65d0cdad8c19d1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5416f1a4f1c1069ce7b7e768eb7368b",
     "grade": false,
     "grade_id": "cell-4676bcfe39c11a07",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 7 (4 Punkte)\n",
    "\n",
    "Entwickle nun dein bestes Ridge-Regression-Modell im Sinne von $R^2$ auf dem Trainingsdatensatz. Du darfst durch Feature-Transformation beliebige weitere Attribute hinzufügen. Gebe $R^2$ und MAE auf dem Testdatensatz an.\n",
    "\n",
    "Zur Optimierung der Hyperparameter kannst du scikit-learn-Funktionalität verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3600764a10f6f837ae5384df5857b2f9",
     "grade": true,
     "grade_id": "cell-c8b2568f340bda1a",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec7af4bd23f868ab39375b2654f983ae",
     "grade": true,
     "grade_id": "cell-dae4f34d368fcdeb",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c48b7faf9e4ff712ee0f8b34df3a1909",
     "grade": false,
     "grade_id": "cell-f039db7c5f629d17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 8 (4 Punkte)\n",
    "\n",
    "Erstelle einen Plot bei welchem du auf der x-Achse die Regularisierungsstärke $\\alpha$ und auf der y-Achse $R^2$ für Trainings- und Testdaten (zwei Kurven) zeichnest. Diskutiere den Plot hinsichtlich Bias-Variance Trade-Off und der Verallgemeinerungsfähigkeit des Modells.  \n",
    "\n",
    "Was ziehst du daraus für Schlüsse für weitere Modellierungsschritte?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cdd2511cc68f17a6d93dad95f9140534",
     "grade": true,
     "grade_id": "cell-3c7f182708f86c2a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d14f655c895df26e63139c013216c54",
     "grade": true,
     "grade_id": "cell-19c5f45de63f73c9",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3782c2247e947870863bd49395f6a8f",
     "grade": false,
     "grade_id": "cell-76d35d0e51f7a3ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 9 (10 Punkte)\n",
    "\n",
    "Was ist das beste Modell für die Output-Variable `price` im Sinne von $R^2$, das du ohne Einschränkungen finden kannst?\n",
    "\n",
    "Vergleiche dazu mindestens drei weitere Ansätze miteinander.\n",
    "\n",
    "Wie verändert sich die Situation wenn du für den *Mean Absolute Error* (MAE) optimierst?\n",
    "\n",
    "Hierzu kannst du scikit-learn Funktionalität verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dafc6ca9eec22c9e7840df949f6180e3",
     "grade": true,
     "grade_id": "cell-76d9178f80e91550",
     "locked": false,
     "points": 8,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "557b204ea14ff8ce0fb40fa3dc0b8d1a",
     "grade": true,
     "grade_id": "cell-f49d6edea9f585c9",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02e93b46e9f8e5a2eb4bb89de79d2bbb",
     "grade": false,
     "grade_id": "cell-deb1a332cd0f2c86",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 10 (4 Punkte)\n",
    "\n",
    "Stelle nun die Resultate der bisherigen Aufgaben und Modelle tabellarisch und graphisch übersichtlich dar und diskutiere sie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f3dfc525169dec929a7e0e67c72b9b1",
     "grade": true,
     "grade_id": "cell-fc430cac16df0d88",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf40ff2cf8567935ebffe998ca4b4db3",
     "grade": true,
     "grade_id": "cell-90051cdc8eaa5d18",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3e906472eee2f6993bee2fee4e52df9",
     "grade": false,
     "grade_id": "cell-e36873df0c0262e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 11 (8 Punkte)\n",
    "\n",
    "Nun betrachten wir noch dein bestes Modell etwas vertieft. Trainiere diesen Modell-Ansatz auf jeweils $[\\frac{1}{10}, \\frac{2}{10}, \\frac{3}{10}, .., \\frac{10}{10}]$ der Trainingsdaten. Erstelle nun einen Plot bei welchem der Wert der Kostenfunktion für die Trainings- und Testdaten auf der y-Achse und der Trainingsdatenanteil auf der x-Achse liegen möge. Zeichne also zwei Kurven in dieses Koordinatensystem.\n",
    "\n",
    "Schau dir dazu das Video von [Kilian Weinberger zu Model Selection](https://www.youtube.com/watch?v=a7cofmFgwIk&list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS&index=22) an.  \n",
    "\n",
    "Interpretiere und diskutiere nun deine Einsichten zu Model Selection, Bias & Variance und Grösse des Datensatzes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48f8a2cbb4285cf7423a6afcf418b578",
     "grade": true,
     "grade_id": "cell-b5a7adbdf7bdd16c",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e62f43ee707f43f2ad20b107985f0f5a",
     "grade": true,
     "grade_id": "cell-f51cd5301642eb12",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
